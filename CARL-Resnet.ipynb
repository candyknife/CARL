{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "25a2ddaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Path Settings\n",
    "img_path = 'data/1-NSCLC-TCIA/3-NSCLC-TCIA-MV/'\n",
    "data_path = 'data/1-NSCLC-TCIA/TCIA-325/'\n",
    "\n",
    "\n",
    "def get_k_fold(k, i):\n",
    "    train_list = None\n",
    "    for j in range(k):\n",
    "        if j == i:\n",
    "            valid_list = np.loadtxt(data_path + 'dataset_fold_' + str(j) + '.txt', delimiter=None, dtype=str)\n",
    "        elif train_list is None:\n",
    "            train_list = np.loadtxt(data_path + 'dataset_fold_' + str(j) + '.txt', delimiter=None, dtype=str)\n",
    "        else:\n",
    "            tmp = np.loadtxt(data_path + 'dataset_fold_' + str(j) + '.txt', delimiter=None, dtype=str)\n",
    "            train_list = np.concatenate((train_list, tmp), axis=0)\n",
    "            tmp = None\n",
    "    return train_list, valid_list\n",
    "\n",
    "\n",
    "def setup_seed(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    \n",
    "setup_seed(2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7a13d90c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset Preparation\n",
    "\n",
    "class Dataset_nsclc(Dataset):\n",
    "    \n",
    "    \n",
    "    def __init__(self, img_path, datalist, is_transform=None):\n",
    "        \n",
    "        self.path = img_path\n",
    "        self.transforms = is_transform\n",
    "\n",
    "        img_name = []\n",
    "        label = []\n",
    "\n",
    "        for line in datalist:\n",
    "            img_name.append(line[0])\n",
    "            label.append(int(line[1]))\n",
    "\n",
    "        label = torch.tensor(label)\n",
    "            \n",
    "        self.name = img_name\n",
    "        self.label = label\n",
    "    \n",
    "    \n",
    "    def get_img(self, index):\n",
    "        data = np.load(self.path + self.name[index] + '.npy')\n",
    "        data = torch.from_numpy(data).type(torch.FloatTensor)\n",
    "        \n",
    "        if self.transforms:\n",
    "            transform = [transforms.RandomHorizontalFlip(), transforms.RandomVerticalFlip()]\n",
    "            transform = transforms.Compose([transforms.ToPILImage(),transforms.RandomChoice(transform),transforms.ToTensor()])\n",
    "            data  = transform(data)\n",
    "\n",
    "        if self.transforms == 'feature':\n",
    "            transform = transforms.Compose([transforms.ToPILImage(),transforms.RandomHorizontalFlip(p=1),transforms.ToTensor()])\n",
    "            data = transform(data)\n",
    "        \n",
    "        return data\n",
    "    \n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        data = self.get_img(index)\n",
    "        data_z = data[:,:,0].unsqueeze(0)\n",
    "        data_y = data[:,:,1].unsqueeze(0)\n",
    "        data_x = data[:,:,2].unsqueeze(0)\n",
    "        return data_z, data_y, data_x, self.label[index], self.name[index]\n",
    "    \n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bfc37d79",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2d6dd693",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tool Functions\n",
    "\n",
    "class CMD(nn.Module):\n",
    "    \"\"\"\n",
    "    Adapted from https://github.com/wzell/cmd/blob/master/models/domain_regularizer.py\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super(CMD, self).__init__()\n",
    "\n",
    "    def forward(self, x1, x2, n_moments):\n",
    "        mx1 = torch.mean(x1, 0)\n",
    "        mx2 = torch.mean(x2, 0)\n",
    "        sx1 = x1-mx1\n",
    "        sx2 = x2-mx2\n",
    "        dm = self.matchnorm(mx1, mx2)\n",
    "        scms = dm\n",
    "        for i in range(n_moments - 1):\n",
    "            scms += self.scm(sx1, sx2, i + 2)\n",
    "        return scms\n",
    "\n",
    "    def matchnorm(self, x1, x2):\n",
    "        power = torch.pow(x1-x2,2)\n",
    "        summed = torch.sum(power)\n",
    "        sqrt = summed**(0.5)\n",
    "        return sqrt\n",
    "        # return ((x1-x2)**2).sum().sqrt()\n",
    "\n",
    "    def scm(self, sx1, sx2, k):\n",
    "        ss1 = torch.mean(torch.pow(sx1, k), 0)\n",
    "        ss2 = torch.mean(torch.pow(sx2, k), 0)\n",
    "        return self.matchnorm(ss1, ss2)\n",
    "\n",
    "\n",
    "class DiffLoss(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(DiffLoss, self).__init__()\n",
    "\n",
    "    def forward(self, input1, input2):\n",
    "\n",
    "        batch_size = input1.size(0)\n",
    "        input1 = input1.view(batch_size, -1)\n",
    "        input2 = input2.view(batch_size, -1)\n",
    "\n",
    "        # Zero mean\n",
    "        input1_mean = torch.mean(input1, dim=0, keepdims=True)\n",
    "        input2_mean = torch.mean(input2, dim=0, keepdims=True)\n",
    "        input1 = input1 - input1_mean\n",
    "        input2 = input2 - input2_mean\n",
    "\n",
    "        input1_l2_norm = torch.norm(input1, p=2, dim=1, keepdim=True).detach()\n",
    "        input1_l2 = input1.div(input1_l2_norm.expand_as(input1) + 1e-6)\n",
    "        \n",
    "\n",
    "        input2_l2_norm = torch.norm(input2, p=2, dim=1, keepdim=True).detach()\n",
    "        input2_l2 = input2.div(input2_l2_norm.expand_as(input2) + 1e-6)\n",
    "\n",
    "        diff_loss = torch.mean((input1_l2.t().mm(input2_l2)).pow(2))\n",
    "\n",
    "        return diff_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "52ecef14",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DoubleConv(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch):\n",
    "        super(DoubleConv, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_ch, out_ch, 3, padding=1),\n",
    "            nn.BatchNorm2d(out_ch),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_ch, out_ch, 3, padding=1),\n",
    "            nn.BatchNorm2d(out_ch),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)\n",
    "\n",
    "class Conv_1_1(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch):\n",
    "        super(Conv_1_1, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_ch, out_ch, 1),\n",
    "            nn.BatchNorm2d(out_ch),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b6ffe0ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Residual(nn.Module):\n",
    "    def __init__(self, input_channels, num_channels,\n",
    "                use_1x1conv=False, strides=1):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(input_channels, num_channels,\n",
    "                              kernel_size=3, padding=1, stride=strides)\n",
    "        self.conv2 = nn.Conv2d(num_channels, num_channels,\n",
    "                              kernel_size=3, padding=1)\n",
    "        if use_1x1conv:\n",
    "            self.conv3 = nn.Conv2d(input_channels, num_channels,\n",
    "                                  kernel_size=1, stride=strides)\n",
    "        else:\n",
    "            self.conv3 = None\n",
    "        self.bn1 = nn.BatchNorm2d(num_channels)\n",
    "        self.bn2 = nn.BatchNorm2d(num_channels)\n",
    "        \n",
    "    def forward(self, X):\n",
    "        Y = F.relu(self.bn1(self.conv1(X)))\n",
    "        Y = self.bn2(self.conv2(Y))\n",
    "        if self.conv3:\n",
    "            X = self.conv3(X)\n",
    "        Y += X\n",
    "        return F.relu(Y)\n",
    "\n",
    "\n",
    "def resnet_block(input_channels, num_channels, num_residuals,\n",
    "                first_block=False):\n",
    "    blk = []\n",
    "    for i in range(num_residuals):\n",
    "        if i == 0 and not first_block:\n",
    "            blk.append(Residual(input_channels, num_channels,\n",
    "                                use_1x1conv=True, strides=2))\n",
    "        else:\n",
    "            blk.append(Residual(num_channels, num_channels))\n",
    "    return blk\n",
    "\n",
    "\n",
    "class Resnet18(nn.Module):\n",
    "    \n",
    "    def __init__(self, block_num = 2):\n",
    "        super().__init__()\n",
    "        # Resnet18\n",
    "        self.reslayer1 = nn.Sequential(nn.Conv2d(1, 64, kernel_size=3, stride=2, padding=1),\n",
    "                                       nn.BatchNorm2d(64), nn.ReLU(),\n",
    "                                       nn.MaxPool2d(kernel_size=3, stride=2, padding=1))\n",
    "        self.reslayer2 = nn.Sequential(*resnet_block(64, 64, block_num, first_block=True))\n",
    "        self.reslayer3 = nn.Sequential(*resnet_block(64, 128, block_num))\n",
    "        self.reslayer4 = nn.Sequential(*resnet_block(128, 256, block_num))\n",
    "        self.reslayer5 = nn.Sequential(*resnet_block(256, 512, block_num))\n",
    "        #self.reslayer6 = nn.Sequential(*resnet_block(512, 1024, block_num))\n",
    "\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Resnet18\n",
    "        x = self.reslayer1(x)\n",
    "        x = self.reslayer2(x)\n",
    "        x = self.reslayer3(x)\n",
    "        x = self.reslayer4(x)\n",
    "        x = self.reslayer5(x)\n",
    "        #x = self.reslayer6(x)\n",
    "        \n",
    "        return x #1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "60da4b2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, in_ch):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.conv_in = Conv_1_1(in_ch, 512)\n",
    "        self.up1 = nn.ConvTranspose2d(512, 256, 2, stride=2)\n",
    "        self.dconv1 = DoubleConv(256, 256)        \n",
    "        self.up2 = nn.ConvTranspose2d(256, 128, 2, stride=2)\n",
    "        self.dconv2 = DoubleConv(128, 128)\n",
    "        self.up3 = nn.ConvTranspose2d(128, 64, 2, stride=2)\n",
    "        self.dconv3 = DoubleConv(64, 64)\n",
    "        self.up4 = nn.ConvTranspose2d(64, 32, 2, stride=2)\n",
    "        self.dconv4 = DoubleConv(32, 32)\n",
    "        self.up5 = nn.ConvTranspose2d(32, 16, 2, stride=2)\n",
    "        self.dconv5 = DoubleConv(16, 16)\n",
    "        self.conv_out = nn.Conv2d(16, 1, 1)\n",
    "        #self.up6 = nn.ConvTranspose2d(16, 8, 2, stride=2)\n",
    "        #self.dconv6 = DoubleConv(8, 8)\n",
    "        #self.conv_out = nn.Conv2d(8, 1, 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv_in(x)                                      \n",
    "        x = self.up1(x)\n",
    "        x = self.dconv1(x)\n",
    "        x = self.up2(x)\n",
    "        x = self.dconv2(x)\n",
    "        x = self.up3(x)\n",
    "        x = self.dconv3(x)\n",
    "        x = self.up4(x)\n",
    "        x = self.dconv4(x)\n",
    "        x = self.up5(x)\n",
    "        x = self.dconv5(x)\n",
    "        #x = self.up6(x)\n",
    "        #x = self.dconv6(x)\n",
    "        out = self.conv_out(x)\n",
    "        out = nn.Sigmoid()(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "378e705a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CARL(nn.Module):\n",
    "    \n",
    "    \n",
    "    def __init__(self, \n",
    "                 block_num = 2, \n",
    "                 dropout = 0.4, \n",
    "                 use_cmd_sim = True,\n",
    "                 loss_weight = [1, 0.1, 1]\n",
    "                ):\n",
    "        super(CARL, self).__init__()\n",
    "        \n",
    "        self.res_block_num = block_num\n",
    "        self.dropout_rate = dropout\n",
    "        self.use_cmd_sim = use_cmd_sim\n",
    "        \n",
    "        self.a = loss_weight[0]\n",
    "        self.b = loss_weight[1]\n",
    "        self.c = loss_weight[2]\n",
    "        \n",
    "        ##########################################\n",
    "        # View-private Encoders\n",
    "        ##########################################\n",
    "        self.encoder_z = Resnet18(block_num)\n",
    "        self.encoder_y = Resnet18(block_num)\n",
    "        self.encoder_x = Resnet18(block_num)\n",
    "        self.encoder_spe = nn.Sequential(*resnet_block(512, 1024, block_num))\n",
    "        \n",
    "        ##########################################\n",
    "        # View-common Encoders\n",
    "        ##########################################\n",
    "        self.encoder_share = Resnet18(block_num)\n",
    "        self.encoder_com = nn.Sequential(*resnet_block(512, 1024, block_num))\n",
    "        \n",
    "        ##########################################\n",
    "        # Decoders\n",
    "        ##########################################\n",
    "        self.decoder_z = Decoder(512)\n",
    "        self.decoder_y = Decoder(512)\n",
    "        self.decoder_x = Decoder(512)\n",
    "        \n",
    "        ##########################################\n",
    "        # Classifier\n",
    "        ##########################################\n",
    "        #input feature size: 8*8*1024\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.out = nn.Sequential(                  \n",
    "                nn.Linear(2048, 256), nn.ReLU(True),nn.Dropout(self.dropout_rate),\n",
    "                nn.Linear(256, 2), nn.Softmax(dim=1)\n",
    "        )  \n",
    "\n",
    "    \n",
    "    def forward(self, data_z, data_y, data_x):\n",
    "\n",
    "        # View-invariant representations\n",
    "        self.z_com = self.encoder_share(data_z)\n",
    "        self.y_com = self.encoder_share(data_y)\n",
    "        self.x_com = self.encoder_share(data_x)\n",
    "        # View-specific representations\n",
    "        self.z_spe = self.encoder_z(data_z)\n",
    "        self.y_spe = self.encoder_y(data_y)\n",
    "        self.x_spe = self.encoder_x(data_x)\n",
    "\n",
    "        # Reconstruction\n",
    "        self.r_z = self.decoder_z(self.z_com + self.z_spe)\n",
    "        self.r_y = self.decoder_y(self.y_com + self.y_spe)\n",
    "        self.r_x = self.decoder_x(self.x_com + self.x_spe)\n",
    "\n",
    "        # Fusion\n",
    "        f_com = torch.cat([self.z_com.unsqueeze(1), self.y_com.unsqueeze(1), self.x_com.unsqueeze(1)], dim=1)\n",
    "        f_com = torch.max(f_com,1)[0]\n",
    "        f_com = self.encoder_com(f_com)\n",
    "        f_com = self.avgpool(f_com).view(f_com.size()[0], -1)\n",
    "\n",
    "        f_spe = torch.cat([self.z_spe.unsqueeze(1), self.y_spe.unsqueeze(1), self.x_spe.unsqueeze(1)], dim=1)\n",
    "        f_spe = torch.max(f_spe,1)[0]\n",
    "        f_spe = self.encoder_com(f_spe)\n",
    "        f_spe = self.avgpool(f_spe).view(f_spe.size()[0], -1)\n",
    "\n",
    "        com_meam = torch.cat([f_com, f_spe], dim=1)\n",
    "\n",
    "        # Classifier\n",
    "        cla_out = self.out(com_meam)\n",
    "\n",
    "        return cla_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4915b89c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from sklearn import metrics\n",
    "\n",
    "def try_gpu(i=0): #@save \n",
    "    if torch.cuda.device_count() >= i + 1: \n",
    "        return torch.device(f'cuda:{i}') \n",
    "    return torch.device('cpu')\n",
    "\n",
    "batch_size = 8\n",
    "DEVICE = try_gpu(0)\n",
    "\n",
    "# Defines training and evaluation.\n",
    "def train_model(model, optimizer, train_loader):\n",
    "    model.train()\n",
    "    \n",
    "    # Loss Functions\n",
    "    loss_cla = nn.CrossEntropyLoss()\n",
    "    loss_recon = nn.MSELoss()\n",
    "    loss_cmd = CMD()\n",
    "    loss_diff = DiffLoss()\n",
    "    \n",
    "    for batch_idx, (data_z,data_y,data_x, y, _) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        data_z,data_y,data_x, y = data_z.to(DEVICE),data_y.to(DEVICE),data_x.to(DEVICE), y.to(DEVICE)\n",
    "        cla_out = model(data_z,data_y,data_x)\n",
    "        \n",
    "        # Loss Calculation\n",
    "        l_cla = loss_cla(cla_out, y)\n",
    "        \n",
    "        l_rec = loss_recon(model.r_z, data_z)\n",
    "        l_rec += loss_recon(model.r_y, data_y)\n",
    "        l_rec += loss_recon(model.r_x, data_x)\n",
    "        l_rec = l_rec/3.0\n",
    "        \n",
    "        l_con = loss_cmd(model.z_com, model.y_com, 5)\n",
    "        l_con += loss_cmd(model.z_com, model.x_com, 5)\n",
    "        l_con += loss_cmd(model.x_com, model.y_com, 5)\n",
    "        l_con = l_con/3.0\n",
    "        \n",
    "        l_dif = loss_diff(model.z_com, model.z_spe)\n",
    "        l_dif += loss_diff(model.y_com, model.y_spe)\n",
    "        l_dif += loss_diff(model.x_com, model.x_spe)\n",
    "        l_dif += loss_diff(model.z_spe, model.y_spe)\n",
    "        l_dif += loss_diff(model.z_spe, model.x_spe)\n",
    "        l_dif += loss_diff(model.x_spe, model.y_spe)\n",
    "        \n",
    "        l = l_cla + model.a*l_rec + model.b*l_con + model.c*l_dif \n",
    "        \n",
    "        #print('batch:{} | l_cla:{:.4f} | l_rec:{:.4f} | l_con:{:.4f} | l_dif:{:.4f}\\n'.format(batch_idx, l_cla, model.a*l_rec, model.b*l_con, model.c*l_dif))\n",
    "        l.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "\n",
    "def eval_model(model, valid_loader):\n",
    "    model.eval()\n",
    "    #\n",
    "    #count = 0\n",
    "    #correct = 0\n",
    "    name = np.array([])\n",
    "    score = np.array([]) \n",
    "    pred = np.array([])\n",
    "    label = np.array([])\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (data_z,data_y,data_x, y, name_batch) in enumerate(valid_loader):\n",
    "            data_z,data_y,data_x, y = data_z.to(DEVICE),data_y.to(DEVICE),data_x.to(DEVICE), y.to(DEVICE)\n",
    "            cla_out = model(data_z,data_y,data_x)\n",
    "            pred_temp = cla_out.argmax(dim=1, keepdim=True)\n",
    "            #correct += pred.eq(y.view_as(pred)).sum().item()\n",
    "            #count += len(y)\n",
    "            name = np.concatenate([name, name_batch], axis=0)\n",
    "            score = np.concatenate([score, cla_out[:,1].cpu().numpy()], axis=0)\n",
    "            pred = np.concatenate([pred, pred_temp.squeeze().cpu().numpy()], axis=0)\n",
    "            label = np.concatenate([label, y.cpu().numpy()], axis=0)\n",
    "            \n",
    "    #acc = correct / count\n",
    "    auc = metrics.roc_auc_score(label, score, average='macro', sample_weight=None)\n",
    "    tn, fp, fn, tp = metrics.confusion_matrix(label, pred).ravel()\n",
    "    acc = (tp + tn) / (tn + fp + fn + tp)\n",
    "    sen = tp / (tp + fn)\n",
    "    spe = tn / (tn + fp)\n",
    "    \n",
    "    return auc, acc, sen, spe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d2d62884",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_manual(fold_num, DEVICE):\n",
    "    \n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    train_list, valid_list = get_k_fold(5, fold_num)\n",
    "    train_data = Dataset_nsclc(img_path, train_list)\n",
    "    valid_data = Dataset_nsclc(img_path, valid_list)\n",
    "    test_data = Dataset_nsclc(test_path, test_list)\n",
    "    train_iter = DataLoader(train_data, batch_size, shuffle=True)\n",
    "    valid_iter = DataLoader(valid_data, batch_size, shuffle=True)\n",
    "    test_iter = DataLoader(test_data, batch_size, shuffle=True)\n",
    "    \n",
    "    \n",
    "    block_num = 2\n",
    "    dropout = 0.3\n",
    "    lr = 1.659-05\n",
    "    epoch = 20\n",
    "    a = 0.01\n",
    "    b = 0.005\n",
    "    c = 83.75\n",
    "    loss_weight = [a, b, c]\n",
    "    \n",
    "    model = CARL(block_num, dropout, loss_weight).to(DEVICE)\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    \n",
    "    #   \n",
    "    result = []\n",
    "    result.append('epoch, tr_auc,tr_acc,tr_sen,tr_spe, va_auc,va_acc,va_sen,va_spe, te_auc,te_acc,te_sen,te_spe')\n",
    "    \n",
    "    for epoch in tqdm(range(epoch)):\n",
    "        \n",
    "        train_model(model, optimizer, train_iter)\n",
    "        \n",
    "        tr_auc, tr_acc, tr_sen, tr_spe = eval_model(model, train_iter)\n",
    "        va_auc, va_acc, va_sen, va_spe = eval_model(model, valid_iter)\n",
    "        te_auc, te_acc, te_sen, te_spe = eval_model(model, test_iter)\n",
    "        \n",
    "        temp = (str(epoch)+','+\\\n",
    "                str(tr_auc)+','+str(tr_acc)+','+str(tr_sen)+','+str(tr_spe)+','+\\\n",
    "                str(va_auc)+','+str(va_acc)+','+str(va_sen)+','+str(va_spe)+','+\\\n",
    "                str(te_auc)+','+str(te_acc)+','+str(te_sen)+','+str(te_spe))\n",
    "        result.append(temp)\n",
    "        \n",
    "        print('epoch:{} | tr_auc:{:.3f} | va_auc:{:.3f} | ts_auc:{:.3f}\\n'.format(epoch, tr_auc, va_auc, te_auc))\n",
    "        print('epoch:{} | tr_acc:{:.3f} | va_acc:{:.3f} | ts_acc:{:.3f}\\n'.format(epoch, tr_acc, va_acc, te_acc))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f55ecde4",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(5):\n",
    "    train_manual(i, DEVICE)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
